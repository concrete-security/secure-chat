services:
  vllm_service:
    image: vllm/vllm-openai:latest
    container_name: vllm_container
    env_file:
        - .env_prod
    gpus: all
    runtime: nvidia
    ports:
      - "${PORT_PROVIDER}:${PORT_PROVIDER}"  # host:container port mapping
    ipc: host
    volumes:
      # - huggingface-cache:/root/.cache/huggingface
      - ./tee:/tee
      - /mnt/disk1/tee_models:/mnt/disk1/tee_models
      - /mnt/disk1/huggingface:/root/.cache/huggingface
    command: >
      --host 0.0.0.0
      --port ${PORT_PROVIDER}
      --model ${MODEL_ID}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION}
      --max-model-len ${VLLM_MAX_MODEL_LEN}
      --max-num-seqs ${VLLM_MAX_NUM_SEQS}
      --max-num-batched-tokens ${VLLM_MAX_NUM_BATCHED_TOKENS}
      --download-dir ${MODEL_STORAGE_DIR}
      --reasoning-parser ${VLLM_REASONING_PARSER}
    healthcheck:
      start_interval: 1h30m # Allow up to 90 minutes for initial model loading
      test: ["CMD", "curl", "-f", "http://localhost:${PORT}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  proxy_api_service:
      build:
        context: ./tee
        dockerfile: Dockerfile.proxy_api
      container_name: proxy_api_container
      depends_on:
        - vllm_service
      environment:
        - VLLM_BASE_URL=http://vllm_service:${PORT_PROVIDER} # internal Docker network URL, use service not container name
      ports:
        - "${PORT_PROXY_RAG}:${PORT_PROXY_RAG}"
      volumes:
        - ./tee:/app
      command: >
        sh -c "uvicorn proxy_app:app --host 0.0.0.0 --port $PORT_PROXY_RAG"

      restart: unless-stopped

  # Nginx reverse proxy with HTTPS
  nginx:
    image: nginx
    container_name: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - tls-certs-keys:/etc/nginx/ssl:ro
    configs:
      - source: nginx-default-conf
        target: /etc/nginx/conf.d/default.conf
        mode: 0444
    networks:
      - vllm
    depends_on:
      cert-manager:
        condition: service_healthy
        restart: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "-k", "https://nginx/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Certificate and key management service
  cert-manager:
    image: ghcr.io/concrete-security/cert-manager:latest
    container_name: cert-manager
    environment:
      - DOMAIN=vllm.concrete-security.com
      - DEV_MODE=true
    volumes:
      - tls-certs-keys:/certs
      - /var/run/dstack.sock:/var/run/dstack.sock
    restart: unless-stopped

networks:
  vllm:
    driver: bridge

volumes:
  # Used to store huggingface models
  huggingface-cache:
  # TLS certificates and keys
  tls-certs-keys:

configs:
  nginx-default-conf:
    content: |
      # Upstream for VLLM service
      upstream vllm_backend {
          server vllm:8000;
      }

      # HTTP server for ACME challenges and redirect
      server {
          listen 80;
          server_name _;

          # ACME challenge endpoint for Let's Encrypt
          # location /.well-known/acme-challenge/ {
          #     root /acme-challenge/;
          #     try_files $$uri =404;
          # }

          # Redirect all other HTTP requests to HTTPS
          location / {
              return 301 https://$$host$$request_uri;
          }
      }

      # HTTPS server
      server {
          listen 443 ssl http2;
          server_name _;

          # SSL certificate and key
          ssl_certificate /etc/nginx/ssl/cert.pem;
          ssl_certificate_key /etc/nginx/ssl/key.pem;

          # ACME challenge endpoint for Let's Encrypt
          # location /.well-known/acme-challenge/ {
          #     root /acme-challenge/;
          #     try_files $$uri =404;
          # }

          # Health check endpoint
          location = /health {
              access_log off;
              return 200 "healthy\n";
              add_header Content-Type text/plain;
          }

          # Proxy to VLLM service
          location / {
              proxy_pass http://vllm_backend;
              proxy_set_header Host $$host;
              proxy_set_header X-Real-IP $$remote_addr;
              proxy_set_header X-Forwarded-For $$proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $$scheme;
          }
      }
