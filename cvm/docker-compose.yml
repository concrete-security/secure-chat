services:
  vllm_service:
    image: vllm/vllm-openai:latest
    container_name: vllm_container
    env_file:
        - .env_prod
    gpus: all
    # runtime: nvidia
    ports:
      - "${PORT_PROVIDER}:${PORT_PROVIDER}"  # host:container port mapping
    ipc: host
    volumes:
      - ./app:/app
      - ${HOST_MODEL_STORAGE_DIR}:${CONTAINER_MODEL_STORAGE_DIR}
      - ${HOST_HF_CACHE}:/root/.cache/huggingface
    # networks:
    #   - vllm
    command: >
      --host 0.0.0.0
      --port ${PORT_PROVIDER}
      --model ${MODEL_ID}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION}
      --max-model-len ${VLLM_MAX_MODEL_LEN}
      --max-num-seqs ${VLLM_MAX_NUM_SEQS}
      --max-num-batched-tokens ${VLLM_MAX_NUM_BATCHED_TOKENS}
      --download-dir ${HOST_MODEL_STORAGE_DIR}
      --reasoning-parser ${VLLM_REASONING_PARSER}
    #--no-enable-prefix-caching
    healthcheck:
      start_period: 1h30m # Allow up to 90 minutes for initial model loading
      test: ["CMD", "curl", "-f", "http://localhost:${PORT_PROVIDER}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  proxy_api_service:
      build:
        context: ./app
        dockerfile: Dockerfile.proxy_api
      container_name: proxy_api_container
      depends_on:
        - vllm_service
      # networks:
      #   - vllm
      environment:
        - VLLM_BASE_URL=http://vllm_service:${PORT_PROVIDER}
      ports:
        - "${PORT_PROXY_RAG}:${PORT_PROXY_RAG}"
      volumes:
        - ./app:/app
      command: >
        sh -c "uvicorn proxy_app:app --host 0.0.0.0 --port $PORT_PROXY_RAG"
      restart: unless-stopped

  # Test: prometheus_service
  # curl http://localhost:9090/targets
  # docker exec -it prometheus_contrainer wget -qO- http://vllm_service:8000/metrics
  # docker exec -it prometheus_contrainer sh -c 'wget -qO- http://vllm_service:8000/metrics | head -n 10'
  prometheus_service:
    image: prom/prometheus:latest
    container_name: prometheus_contrainer
    # networks:
    #   - vllm
    volumes:
      - ../monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../monitoring/prom-data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
    restart: unless-stopped

  # Test: grafana_service
  # docker exec -it grafana_container sh
  # ls -R /etc/grafana/provisioning
  # docker logs grafana_container | grep -i 'provision' | head -n 2
  # docker exec -it grafana_container sh -c 'wget -qO- http://prometheus_service:9090/-/ready'
  grafana_service:
    image: grafana/grafana:latest
    container_name: grafana_container
    # networks:
    #   - vllm
    depends_on:
      - prometheus_service
    ports:
      - "4000:4000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SERVER_HTTP_PORT=4000
      - GF_SERVER_HTTP_ADDR=0.0.0.0
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/etc/grafana/provisioning/dashboards/vllm.json
    volumes:
      # Mount: dashboards and datasources
      - ../monitoring/grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped

  # Nginx reverse proxy with HTTPS
  nginx:
    image: nginx
    container_name: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - tls-certs-keys:/etc/nginx/ssl:ro
    configs:
      - source: nginx-default-conf
        target: /etc/nginx/conf.d/default.conf
        mode: 0444
    # networks:
    #   - vllm
    depends_on:
      cert-manager:
        condition: service_healthy
        restart: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "-k", "https://nginx/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Certificate and key management service
  cert-manager:
    image: ghcr.io/concrete-security/cert-manager:latest
    container_name: cert-manager
    environment:
      - DOMAIN=vllm.concrete-security.com
      - DEV_MODE=true
    volumes:
      - tls-certs-keys:/certs
      - /var/run/dstack.sock:/var/run/dstack.sock
    restart: unless-stopped

# docker network inspect vllm --format '{{json .Containers}}' | jq .
networks:
  default:
    name: vllm
    driver: bridge

volumes:
  # Used to store huggingface models
  huggingface-cache:
  # TLS certificates and keys
  tls-certs-keys:

configs:
  nginx-default-conf:
    content: |
      # Upstream for VLLM service
      upstream vllm_backend {
          server vllm_service:${PORT_PROVIDER};
      }

      # HTTP server for ACME challenges and redirect
      server {
          listen 80;
          server_name _;

          # ACME challenge endpoint for Let's Encrypt
          # location /.well-known/acme-challenge/ {
          #     root /acme-challenge/;
          #     try_files $$uri =404;
          # }

          # Redirect all other HTTP requests to HTTPS
          location / {
              return 301 https://$$host$$request_uri;
          }
      }

      # HTTPS server
      server {
          listen 443 ssl http2;
          server_name _;

          # SSL certificate and key
          ssl_certificate /etc/nginx/ssl/cert.pem;
          ssl_certificate_key /etc/nginx/ssl/key.pem;

          # ACME challenge endpoint for Let's Encrypt
          # location /.well-known/acme-challenge/ {
          #     root /acme-challenge/;
          #     try_files $$uri =404;
          # }

          # Health check endpoint
          location = /health {
              access_log off;
              return 200 "healthy\n";
              add_header Content-Type text/plain;
          }

          # Proxy to VLLM service
          location / {
              proxy_pass http://vllm_backend;
              proxy_set_header Host $$host;
              proxy_set_header X-Real-IP $$remote_addr;
              proxy_set_header X-Forwarded-For $$proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $$scheme;
          }
      }
